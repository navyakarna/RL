{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsbk6zUt3pJtwYJpCHbgSV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navyakarna/RL/blob/master/Untitled23.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "4h4hg5uxyZ-D",
        "outputId": "5de0f484-7ec0-419c-b0d0-be0eb0677d0e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-027217d1e3ef>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_rows\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_cols\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Check if the new state is within bounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mnew_state_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_action_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mnew_state_max_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_state_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnew_state_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             q_table[state_action_to_index(state, action)] += alpha * (\n\u001b[1;32m     60\u001b[0m                     reward + gamma * new_state_max_q - q_table[state_action_to_index(state, action)])\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2791\u001b[0m     \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2792\u001b[0m     \"\"\"\n\u001b[0;32m-> 2793\u001b[0;31m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[1;32m   2794\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   2795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Define the environment (grid world)\n",
        "# 'S' represents the starting point\n",
        "# 'G' represents the goal\n",
        "# 'H' represents a hazardous state\n",
        "# 'F' represents a safe state\n",
        "# The agent can move in the directions: UP, DOWN, LEFT, RIGHT\n",
        "env = np.array([['S', 'F', 'F', 'H'],\n",
        "                ['F', 'H', 'F', 'H'],\n",
        "                ['F', 'F', 'F', 'H'],\n",
        "                ['H', 'F', 'F', 'G']])\n",
        "\n",
        "# Define parameters\n",
        "num_episodes = 1000\n",
        "alpha = 0.1  # learning rate\n",
        "gamma = 0.9  # discount factor\n",
        "epsilon = 0.1  # exploration-exploitation trade-off\n",
        "\n",
        "# Extract the size of the grid world\n",
        "num_rows, num_cols = env.shape\n",
        "\n",
        "# Map state-action pairs to indices in the Q-table\n",
        "def state_action_to_index(state, action):\n",
        "    return (state[0] * num_cols + state[1]) * 4 + action\n",
        "\n",
        "# Initialize Q-table with zeros\n",
        "q_table = np.zeros((num_rows * num_cols, 4))\n",
        "\n",
        "# Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    # Reset the environment to the starting state\n",
        "    state = np.argwhere(env == 'S')[0]\n",
        "\n",
        "    # Episode loop\n",
        "    while True:\n",
        "        # Choose action using epsilon-greedy policy\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(4)  # Explore\n",
        "        else:\n",
        "            action = np.argmax(q_table[state_action_to_index(state, 0):state_action_to_index(state, 4)])  # Exploit\n",
        "\n",
        "        # Take the chosen action and observe the next state and reward\n",
        "        if action == 0:  # UP\n",
        "            new_state = (max(state[0] - 1, 0), state[1])\n",
        "        elif action == 1:  # DOWN\n",
        "            new_state = (min(state[0] + 1, num_rows - 1), state[1])\n",
        "        elif action == 2:  # LEFT\n",
        "            new_state = (state[0], max(state[1] - 1, 0))\n",
        "        else:  # RIGHT\n",
        "            new_state = (state[0], min(state[1] + 1, num_cols - 1))\n",
        "\n",
        "        reward = -1 if env[new_state] == 'F' else 10 if env[new_state] == 'G' else -10  # Define rewards\n",
        "\n",
        "        # Update Q-value using the Q-learning update rule\n",
        "        if new_state[0] < num_rows and new_state[1] < num_cols:  # Check if the new state is within bounds\n",
        "            new_state_index = state_action_to_index(new_state, 0)\n",
        "            new_state_max_q = np.max(q_table[new_state_index:new_state_index + 4])\n",
        "            q_table[state_action_to_index(state, action)] += alpha * (\n",
        "                    reward + gamma * new_state_max_q - q_table[state_action_to_index(state, action)])\n",
        "\n",
        "        # Move to the next state\n",
        "        state = new_state\n",
        "\n",
        "        # Check if the goal is reached or hazardous state is encountered\n",
        "        if env[state] in ['G', 'H']:\n",
        "            break\n",
        "\n",
        "# Display the learned Q-table\n",
        "print(\"Learned Q-table:\")\n",
        "print(q_table)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the environment (grid world)\n",
        "env = np.array([['S', 'F', 'F', 'H'],\n",
        "                ['F', 'H', 'F', 'H'],\n",
        "                ['F', 'F', 'F', 'H'],\n",
        "                ['H', 'F', 'F', 'G']])\n",
        "\n",
        "# Define parameters\n",
        "num_episodes = 1000\n",
        "alpha = 0.1  # learning rate\n",
        "gamma = 0.9  # discount factor\n",
        "epsilon = 0.1  # exploration-exploitation trade-off\n",
        "\n",
        "# Extract the size of the grid world\n",
        "num_rows, num_cols = env.shape\n",
        "\n",
        "# Map state-action pairs to indices in the Q-table\n",
        "def state_action_to_index(state, action):\n",
        "    return (state[0] * num_cols + state[1]) * 4 + action\n",
        "\n",
        "# Initialize Q-table with zeros\n",
        "q_table = np.zeros((num_rows * num_cols, 4))\n",
        "\n",
        "# Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    # Reset the environment to the starting state\n",
        "    state = np.argwhere(env == 'S')[0]\n",
        "\n",
        "    # Episode loop\n",
        "    while True:\n",
        "        # Choose action using epsilon-greedy policy\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(4)  # Explore\n",
        "        else:\n",
        "            action = np.argmax(q_table[state_action_to_index(state, 0):state_action_to_index(state, 4)])  # Exploit\n",
        "\n",
        "        # Take the chosen action and observe the next state and reward\n",
        "        if action == 0:  # UP\n",
        "            new_state = (max(state[0] - 1, 0), state[1])\n",
        "        elif action == 1:  # DOWN\n",
        "            new_state = (min(state[0] + 1, num_rows - 1), state[1])\n",
        "        elif action == 2:  # LEFT\n",
        "            new_state = (state[0], max(state[1] - 1, 0))\n",
        "        else:  # RIGHT\n",
        "            new_state = (state[0], min(state[1] + 1, num_cols - 1))\n",
        "\n",
        "        reward = -1 if env[new_state] == 'F' else 10 if env[new_state] == 'G' else -10  # Define rewards\n",
        "\n",
        "        # Update Q-value using the Q-learning update rule\n",
        "        new_state_index = state_action_to_index(new_state, 0)\n",
        "        new_state_max_q = np.max(q_table[new_state_index:new_state_index + 4])\n",
        "        q_table[state_action_to_index(state, action)] += alpha * (\n",
        "                reward + gamma * new_state_max_q - q_table[state_action_to_index(state, action)])\n",
        "\n",
        "        # Move to the next state\n",
        "        state = new_state\n",
        "\n",
        "        # Check if the goal is reached or hazardous state is encountered\n",
        "        if env[state] in ['G', 'H']:\n",
        "            break\n",
        "\n",
        "# Display the learned Q-table\n",
        "print(\"Learned Q-table:\")\n",
        "print(q_table)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "n3HVxrM8zG0n",
        "outputId": "5a241219-f86a-4ac6-e2a9-1014d6320cc1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ef1c84c0c7be>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Update Q-value using the Q-learning update rule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mnew_state_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_action_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mnew_state_max_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_state_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnew_state_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         q_table[state_action_to_index(state, action)] += alpha * (\n\u001b[1;32m     54\u001b[0m                 reward + gamma * new_state_max_q - q_table[state_action_to_index(state, action)])\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2791\u001b[0m     \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2792\u001b[0m     \"\"\"\n\u001b[0;32m-> 2793\u001b[0;31m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[1;32m   2794\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   2795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the environment (grid world)\n",
        "env = np.array([['S', 'F', 'F', 'H'],\n",
        "                ['F', 'H', 'F', 'H'],\n",
        "                ['F', 'F', 'F', 'H'],\n",
        "                ['H', 'F', 'F', 'G']])\n",
        "\n",
        "# Define parameters\n",
        "num_episodes = 1000\n",
        "alpha = 0.1  # learning rate\n",
        "gamma = 0.9  # discount factor\n",
        "epsilon = 0.1  # exploration-exploitation trade-off\n",
        "\n",
        "# Extract the size of the grid world\n",
        "num_rows, num_cols = env.shape\n",
        "\n",
        "# Map state-action pairs to indices in the Q-table\n",
        "def state_action_to_index(state, action):\n",
        "    return (state[0] * num_cols + state[1]) * 4 + action\n",
        "\n",
        "# Initialize Q-table with zeros\n",
        "q_table = np.zeros((num_rows * num_cols, 4))\n",
        "\n",
        "# Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    # Reset the environment to the starting state\n",
        "    state = np.argwhere(env == 'S')[0]\n",
        "\n",
        "    # Episode loop\n",
        "    while True:\n",
        "        # Choose action using epsilon-greedy policy\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(4)  # Explore\n",
        "        else:\n",
        "            action = np.argmax(q_table[state_action_to_index(state, 0):state_action_to_index(state, 4)])  # Exploit\n",
        "\n",
        "        # Take the chosen action and observe the next state and reward\n",
        "        if action == 0:  # UP\n",
        "            new_state = (max(state[0] - 1, 0), state[1])\n",
        "        elif action == 1:  # DOWN\n",
        "            new_state = (min(state[0] + 1, num_rows - 1), state[1])\n",
        "        elif action == 2:  # LEFT\n",
        "            new_state = (state[0], max(state[1] - 1, 0))\n",
        "        else:  # RIGHT\n",
        "            new_state = (state[0], min(state[1] + 1, num_cols - 1))\n",
        "\n",
        "        reward = -1 if env[new_state] == 'F' else 10 if env[new_state] == 'G' else -10  # Define rewards\n",
        "\n",
        "        # Update Q-value using the Q-learning update rule\n",
        "        new_state_index = state_action_to_index(new_state, 0)\n",
        "        new_state_max_q = np.max(q_table[new_state_index:new_state_index + 4])\n",
        "        q_table[state_action_to_index(state, action)] += alpha * (\n",
        "                reward + gamma * new_state_max_q - q_table[state_action_to_index(state, action)])\n",
        "\n",
        "        # Move to the next state\n",
        "        state = new_state\n",
        "\n",
        "        # Check if the goal is reached or hazardous state is encountered\n",
        "        if env[state] in ['G', 'H']:\n",
        "            break\n",
        "\n",
        "# Display the learned Q-table\n",
        "print(\"Learned Q-table:\")\n",
        "print(q_table)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "Odmx6bTxzKk7",
        "outputId": "bd2cc3d8-6b70-4544-8056-88b4dcae467b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ef1c84c0c7be>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Update Q-value using the Q-learning update rule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mnew_state_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_action_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mnew_state_max_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_state_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnew_state_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         q_table[state_action_to_index(state, action)] += alpha * (\n\u001b[1;32m     54\u001b[0m                 reward + gamma * new_state_max_q - q_table[state_action_to_index(state, action)])\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2791\u001b[0m     \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2792\u001b[0m     \"\"\"\n\u001b[0;32m-> 2793\u001b[0;31m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[1;32m   2794\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   2795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the environment (grid world)\n",
        "env = np.array([['S', 'F', 'F', 'H'],\n",
        "                ['F', 'H', 'F', 'H'],\n",
        "                ['F', 'F', 'F', 'H'],\n",
        "                ['H', 'F', 'F', 'G']])\n",
        "\n",
        "# Define parameters\n",
        "num_episodes = 1000\n",
        "alpha = 0.1  # learning rate\n",
        "gamma = 0.9  # discount factor\n",
        "epsilon = 0.1  # exploration-exploitation trade-off\n",
        "\n",
        "# Extract the size of the grid world\n",
        "num_rows, num_cols = env.shape\n",
        "\n",
        "# Map state-action pairs to indices in the Q-table\n",
        "def state_action_to_index(state, action):\n",
        "    return (state[0] * num_cols + state[1]) * 4 + action\n",
        "\n",
        "# Initialize Q-table with zeros\n",
        "q_table = np.zeros((num_rows * num_cols, 4))\n",
        "\n",
        "# Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    # Reset the environment to the starting state\n",
        "    state = np.argwhere(env == 'S')[0]\n",
        "\n",
        "    # Episode loop\n",
        "    while True:\n",
        "        # Choose action using epsilon-greedy policy\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(4)  # Explore\n",
        "        else:\n",
        "            action = np.argmax(q_table[state_action_to_index(state, 0):state_action_to_index(state, 4)])  # Exploit\n",
        "\n",
        "        # Take the chosen action and observe the next state and reward\n",
        "        if action == 0:  # UP\n",
        "            next_state = (max(state[0] - 1, 0), state[1])\n",
        "        elif action == 1:  # DOWN\n",
        "            next_state = (min(state[0] + 1, num_rows - 1), state[1])\n",
        "        elif action == 2:  # LEFT\n",
        "            next_state = (state[0], max(state[1] - 1, 0))\n",
        "        else:  # RIGHT\n",
        "            next_state = (state[0], min(state[1] + 1, num_cols - 1))\n",
        "\n",
        "        reward = -1 if env[next_state] == 'F' else 10 if env[next_state] == 'G' else -10  # Define rewards\n",
        "\n",
        "        # Update Q-value using the Q-learning update rule\n",
        "        q_table[state_action_to_index(state, action)] += alpha * (\n",
        "                reward + gamma * np.max(q_table[state_action_to_index(next_state, 0):state_action_to_index(next_state, 4)]) - q_table[state_action_to_index(state, action)])\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Check if the goal is reached or hazardous state is encountered\n",
        "        if env[state] in ['G', 'H']:\n",
        "            break\n",
        "\n",
        "# Display the learned Q-table\n",
        "print(\"Learned Q-table:\")\n",
        "print(q_table)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "8XUIZa2FzSPM",
        "outputId": "a25be2bc-4ed5-431e-ef47-00db356dc5e1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e5ce152c77b6>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Update Q-value using the Q-learning update rule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         q_table[state_action_to_index(state, action)] += alpha * (\n\u001b[0;32m---> 52\u001b[0;31m                 reward + gamma * np.max(q_table[state_action_to_index(next_state, 0):state_action_to_index(next_state, 4)]) - q_table[state_action_to_index(state, action)])\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Move to the next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2791\u001b[0m     \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2792\u001b[0m     \"\"\"\n\u001b[0;32m-> 2793\u001b[0;31m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[1;32m   2794\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   2795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the environment (grid world)\n",
        "env = np.array([['S', 'F', 'F', 'H'],\n",
        "                ['F', 'H', 'F', 'H'],\n",
        "                ['F', 'F', 'F', 'H'],\n",
        "                ['H', 'F', 'F', 'G']])\n",
        "\n",
        "# Define parameters\n",
        "num_episodes = 1000\n",
        "alpha = 0.1  # learning rate\n",
        "gamma = 0.9  # discount factor\n",
        "epsilon = 0.1  # exploration-exploitation trade-off\n",
        "\n",
        "# Extract the size of the grid world\n",
        "num_rows, num_cols = env.shape\n",
        "\n",
        "# Map state-action pairs to indices in the Q-table\n",
        "def state_action_to_index(state, action):\n",
        "    return (state[0] * num_cols + state[1]) * 4 + action\n",
        "\n",
        "# Initialize Q-table with zeros\n",
        "q_table = np.zeros((num_rows * num_cols, 4))\n",
        "\n",
        "# Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    # Reset the environment to the starting state\n",
        "    state = np.argwhere(env == 'S')[0]\n",
        "\n",
        "    # Episode loop\n",
        "    while True:\n",
        "        # Choose action using epsilon-greedy policy\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(4)  # Explore\n",
        "        else:\n",
        "            action = np.argmax(q_table[state_action_to_index(state, 0):state_action_to_index(state, 4)])  # Exploit\n",
        "\n",
        "        # Take the chosen action and observe the next state and reward\n",
        "        if action == 0:  # UP\n",
        "            next_state = (max(state[0] - 1, 0), state[1])\n",
        "        elif action == 1:  # DOWN\n",
        "            next_state = (min(state[0] + 1, num_rows - 1), state[1])\n",
        "        elif action == 2:  # LEFT\n",
        "            next_state = (state[0], max(state[1] - 1, 0))\n",
        "        else:  # RIGHT\n",
        "            next_state = (state[0], min(state[1] + 1, num_cols - 1))\n",
        "\n",
        "        reward = -1 if env[next_state] == 'F' else 10 if env[next_state] == 'G' else -10  # Define rewards\n",
        "\n",
        "        # Update Q-value using the Q-learning update rule\n",
        "        q_table[state_action_to_index(state, action)] += alpha * (\n",
        "                reward + gamma * np.max(q_table[state_action_to_index(next_state, 0):state_action_to_index(next_state, 4)]) - q_table[state_action_to_index(state, action)])\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Check if the goal is reached or hazardous state is encountered\n",
        "        if env[state] in ['G', 'H']:\n",
        "            break\n",
        "\n",
        "# Display the learned Q-table\n",
        "print(\"Learned Q-table:\")\n",
        "print(q_table)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "zCl-Bor5zaBH",
        "outputId": "d7b5c79d-defd-4e6e-d1a0-9b86602a89d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fcce44aea47b>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Update Q-value using the Q-learning update rule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         q_table[state_action_to_index(state, action)] += alpha * (\n\u001b[0;32m---> 52\u001b[0;31m                 reward + gamma * np.max(q_table[state_action_to_index(next_state, 0):state_action_to_index(next_state, 4)]) - q_table[state_action_to_index(state, action)])\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Move to the next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2791\u001b[0m     \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2792\u001b[0m     \"\"\"\n\u001b[0;32m-> 2793\u001b[0;31m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[1;32m   2794\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   2795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym as GYM\n",
        "import itertools as IT\n",
        "import matplotlib as MPLOT\n",
        "import matplotlib.style as MPLOTS\n",
        "import numpy as nmp\n",
        "import pandas as pnd\n",
        "import sys\n",
        "from collections import defaultdict as DD"
      ],
      "metadata": {
        "id": "1ixB4hjlzlZ0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v1\")\n",
        "n_observations1 = env.observation_space.n\n",
        "n_actions1 = env.action_space.n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "u6mEUx9wzmbG",
        "outputId": "10b30b85-6e9b-429a-b716-8cf40195882d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2b486f311fae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FrozenLake-v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mn_observations1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn_actions1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sEOsvlkM1Ymb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "d55vjHLizphR",
        "outputId": "e1554662-87b9-4daa-b9a2-5175133359f0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2aae23bd7c04>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMouse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcellular\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAgent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mcolour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gray'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mai\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cellular' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the environment\n",
        "num_states = 6\n",
        "num_actions = 2\n",
        "gamma = 0.8  # Discount factor\n",
        "\n",
        "# Initialize Q-table with zeros\n",
        "Q = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Define the reward matrix\n",
        "R = np.array([\n",
        "    [-1, -1, -1, -1, 0, -1],\n",
        "    [-1, -1, -1, 0, -1, 100],\n",
        "    [-1, -1, -1, 0, -1, -1],\n",
        "    [-1, 0, 0, -1, 0, -1],\n",
        "    [0, -1, -1, 0, -1, 100],\n",
        "    [-1, 0, -1, -1, 0, 100]\n",
        "])\n",
        "\n",
        "# Perform Q-learning\n",
        "num_episodes = 1000\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    current_state = np.random.randint(0, num_states)\n",
        "\n",
        "    while current_state != 1:  # Continue until the agent reaches the goal state\n",
        "        possible_actions = np.where(R[current_state, :] >= 0)[0]  # Get valid actions for the current state\n",
        "        action = np.random.choice(possible_actions)  # Randomly choose an action\n",
        "\n",
        "        next_state = action  # In this example, the next state is determined by the action taken\n",
        "        future_rewards = Q[next_state, :]\n",
        "        Q[current_state, action] = R[current_state, action] + gamma * np.max(future_rewards)\n",
        "\n",
        "        current_state = next_state\n",
        "\n",
        "# Display the learned Q-table\n",
        "print(\"Learned Q-table:\")\n",
        "print(Q)\n",
        "\n",
        "# Test the learned policy\n",
        "current_state = 0\n",
        "steps = [current_state]\n",
        "\n",
        "while current_state != 1:\n",
        "    action = np.argmax(Q[current_state, :])\n",
        "    current_state = action\n",
        "    steps.append(current_state)\n",
        "\n",
        "print(\"Optimal path:\")\n",
        "print(steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "MQhpflPf1ZZr",
        "outputId": "e2aae1c5-fad7-47a4-d4e4-a6e64ccc5be6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-7768f200d99d>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m  \u001b[0;31m# In this example, the next state is determined by the action taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mfuture_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 1 with size 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MarkovProcess:\n",
        "    def __init__(self, transition_matrix, initial_state):\n",
        "        self.transition_matrix = np.array(transition_matrix)\n",
        "        self.current_state = initial_state\n",
        "\n",
        "    def step(self):\n",
        "        # Use the transition matrix to determine the next state\n",
        "        probabilities = self.transition_matrix[self.current_state, :]\n",
        "        next_state = np.random.choice(len(probabilities), p=probabilities)\n",
        "        self.current_state = next_state\n",
        "        return next_state\n",
        "\n",
        "# Example usage\n",
        "transition_matrix = [\n",
        "    [0.7, 0.3],  # From state 0 to states 0 and 1\n",
        "    [0.2, 0.8],  # From state 1 to states 0 and 1\n",
        "]\n",
        "\n",
        "initial_state = 0\n",
        "num_steps = 10\n",
        "\n",
        "markov_process = MarkovProcess(transition_matrix, initial_state)\n",
        "\n",
        "print(\"Initial State:\", initial_state)\n",
        "for _ in range(num_steps):\n",
        "    next_state = markov_process.step()\n",
        "    print(\"Next State:\", next_state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a27sBsgQ14E3",
        "outputId": "a60bfde2-0303-4924-da3c-90409500a8e5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial State: 0\n",
            "Next State: 1\n",
            "Next State: 1\n",
            "Next State: 1\n",
            "Next State: 1\n",
            "Next State: 1\n",
            "Next State: 0\n",
            "Next State: 1\n",
            "Next State: 1\n",
            "Next State: 0\n",
            "Next State: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Create Frozen Lake environment\n",
        "env = gym.make('FrozenLake-v1', is_slippery=False)  # is_slippery=False for deterministic transitions\n",
        "\n",
        "# Define parameters\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "alpha = 0.1  # learning rate\n",
        "gamma = 0.99  # discount factor\n",
        "num_episodes = 1000\n",
        "\n",
        "# Initialize value function\n",
        "V = np.zeros(num_states)\n",
        "\n",
        "# TD(0) algorithm\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Choose an action using a simple policy (e.g., epsilon-greedy)\n",
        "        action = np.argmax(V[state])\n",
        "\n",
        "        # Take the chosen action and observe the next state and reward\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Update the value function using the TD(0) update rule\n",
        "        V[state] += alpha * (reward + gamma * V[next_state] - V[state])\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "# Print the learned value function\n",
        "print(\"Learned Value Function:\")\n",
        "print(V.reshape((4, 4)))  # Reshape for better readability in the 4x4 grid of Frozen Lake\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo91Nl_N2b-M",
        "outputId": "438cbf38-c943-48ca-95a5-fc9aa1853550"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Value Function:\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    }
  ]
}